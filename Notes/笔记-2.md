###  02 AI Agent
***
### 目录
- [1 . AI Agent 基本原理](#1-ai-agent-基本原理)
  - [1.1 AI Agent特性](#11-ai-agent特性)
  - [1.2 如何打造](#12-如何打造)
    - [1.2.1 强化学习RL（`Reinforcement Learning`）](#121-强化学习rlreinforcement-learning)
    - [1.2.2 大语言模型LLM（`Large Language Model`）](#122-大语言模型llmlarge-language-model)
- [2 . AI Agent关键能力剖析](#2-ai-agent关键能力剖析)
  - [2.1 AI如何根据经验调整行为](#21-ai如何根据经验调整行为)
  - [2.2 AI如何使用工具](#22-ai如何使用工具)
    - [2.2.1 语言模型常用工具](#221-语言模型常用工具)
    - [2.2.2 使用工具过程](#222-使用工具过程)
    - [2.2.3 常见问题](#223-常见问题)
  - [2.3 AI能不能做计划](#23-ai能不能做计划)
    - [2.3.1 做计划流程](#231-做计划流程)
    - [2.3.2 如何强化AI Agent的规划能力](#232-如何强化ai-agent的规划能力)



### 1. AI Agent 基本原理
#### 1.1 AI Agent特性
- 以往使用AI是通过人类下达明确指令，一个口令使得AI完成一个动作. 而AI Agent是人类给予其一定目标或研究某项问题，其自行进行假设、实验、分析等过程
<img src="https://i-blog.csdnimg.cn/direct/258bf2c7323d4b56a2ded15842165965.png">
- 基本流程图：在给定目标（`Goal`）后，AI Agent会对周围环境进行观察（`Observation`）后采取行动（`Action`），而行动又会改变环境，使得机器会根据改变的环境采取新的行动，如此循环往复
<img src="https://i-blog.csdnimg.cn/direct/d70a68b963c440dc9aa52c0e4e7cdd89.png">

#### 1.2 如何打造
##### 1.2.1 强化学习RL（`Reinforcement Learning`）
过去往往采用RL演算法打造AI Agent，通过`Reward`奖励机制来衡量goal，所训练的AI Agent就会Learn to Maximize Reward. 但**局限**是需要每一个任务都用RL训练模型
##### 1.2.2 大语言模型LLM（`Large Language Model`）
- 将goal和observation转化为文字描述（`option`）传递给模型，同时也以文字形式产生action再转译成对应行动
<img src="https://i-blog.csdnimg.cn/direct/d5852742d2d64d489b79ea1f7720d0fc.png">
- 相较于传统的agent，LLM不需要引入reward，可以使agent更容易按照环境的状态和回馈等调整自身行为，更加灵活.
<img src="https://i-blog.csdnimg.cn/direct/55c5a77a0cf7467fb2d3a2a68871840c.png">
- 且LLM能改变以往的回合制互动，实现更加真实的即时互动（即action1未执行完时，observation就已改变，能迅速转变为action2）
<img src="https://i-blog.csdnimg.cn/direct/719a116d4272417282cb22f071119c1d.png">

### 2 AI Agent关键能力剖析
#### 2.1 AI如何根据经验调整行为
- 可以把经验存储起来，让模型每次行动都根据过往经验来调整行为。  
  但局限是，当执行次数变大时，由于每次行动都需考虑之前经验，可能导致算力不够
  <img src="https://i-blog.csdnimg.cn/direct/3daf9960b1f94f19ba5068acdf5ee5c6.png">
- 解决方案：将经验存入记忆模块（`Menmory`），引入read模组（检索系统），write模组（筛选值得记忆内容）和reflection模组（对记忆整合，也可制作`Knowledge Graph`）.如此，在遇到observation时只从记忆模块中选择与问题相关的经验
<img src="https://i-blog.csdnimg.cn/direct/9253359b852d4406908732d2f6242462.png">

#### 2.2 AI如何使用工具
##### 2.2.1 语言模型常用工具
- Search Engine
- 程序
- Other AI（Different Capabilities）
>工具可以看作是function，使用工具就是在调用这些function（`Function Call`）
##### 2.2.2 使用工具过程
- 使用工具需提供`Prompt`，prompt分为`System Prompt`和`User Prompt`
system prompt一般不变，是放在模型最前面，一般是在开发过程中的prompt，给模型提供如何使用工具和使用特定工具的方法.而user prompt输入会改变，一般是使用者提供的prompt
<img src="https://i-blog.csdnimg.cn/direct/bffd6fe3146c4b218b8e4b2ac1d5e297.png">

- 但输出毕竟只是一段文字，需agent开发者设定特定流程，让机器按步骤进行，并设定排除掉不需呈现的内容、生成需呈现内容
<img src="https://i-blog.csdnimg.cn/direct/bc7e9722db804bdc9a31d5e24d6e2e17.png">

- ai有时会调用内部api，使用其他ai作为工具来打出“组合技”
<img src="https://i-blog.csdnimg.cn/direct/365d87858c9d4b08ae93ef80f51616e5.png">

##### 2.2.3 常见问题
Q:工具过多怎么办？
A:可以同样构建一个memory作为工具包，根据不同情况选定特定工具（模型甚至可以自己打造工具到工具包内）
<img src="https://i-blog.csdnimg.cn/direct/09dca523268f42a088bb6f5bfc1561c7.png">
Q：语言模型有没有自己的判断能力？
A：有，语言模型在使用工具或RAG时，有自己的internal knowledge.它从工具获得external knowledge，二者差距越大，模型就越不相信工具提供的外部知识。且同一个问题，模型更倾向于相信ai同类提供的答案而不是人类提供的
Q：如果工具可靠，ai一定不会出错吗？
A：不是，就算找到所有资料都是对的，不保证答案就是对的（往年ai在出现重名问题时，会把两人混淆成同一个人，目前该问题已解决）

#### 2.3 AI能不能做计划
##### 2.3.1 做计划流程
- ai先通过给定的obs1创造出一个plan，若顺利则一路执行下去.但在obs1适用的plan，在下一时刻obs2不一定适用.理想的做法是让ai在每个obs都创造出plan2加入到这个sequence中
<img src="https://i-blog.csdnimg.cn/direct/c610b94e31cb4139969c2fb71d4cfade.png">

##### 2.3.2 如何强化AI Agent的规划能力 
- 可以通过爆搜所有路径来找出最优解，但是很明显的缺点是，路径一旦长起来，所需要算力以指数级增长，且得到的结果也不一定是切实际的做法
<img src="https://i-blog.csdnimg.cn/direct/87c05ab5b0b04fdf8ab03fecd7fddde6.png">

- 基于以上方法的优化是模型给每一步行动赋予可能实现的概率，把一些低于threshold方向给舍弃（类似于剪枝），这种方法叫作`Tree Search`.其缺点是有些动作（如订餐）无法回溯
<img src="https://i-blog.csdnimg.cn/direct/202a526f38364e02bb58d990eca3181f.png">
- 为了解决无法回溯的问题，则需要`World Model`来模拟环境可能会有的变化，可以让ai自己扮演World Model来找出所有可能的情况和最优解.
<img src="https://i-blog.csdnimg.cn/direct/be65c47aae9f4e60925ec68aee1efafd.png">