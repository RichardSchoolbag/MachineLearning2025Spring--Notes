### 08 模型编辑
***
### 目录
- [08 模型编辑](#08-模型编辑)
- [目录](#目录)
- [1. Model Editing与Post-training区别](#1-model-editing与post-training区别)
- [2. 评估Model Editing的三个维度](#2-评估model-editing的三个维度)
  - [2.1 Generalization的界定](#21-generalization的界定)
- [3. Model Editing常见方法](#3-model-editing常见方法)
  - [3.1 不改参数（`IKE`）](#31-不改参数ike)
  - [3.2 修改参数](#32-修改参数)
    - [3.2.1 人类决定的编辑（`ROME`）](#321-人类决定的编辑rome)
    - [3.2.2 人工智能学习如何编辑](#322-人工智能学习如何编辑)
### 1. Model Editing与Post-training区别
- Model Editing一般是把某个特定的知识植入给模型，而Post-training则是让模型学会某项技能.理论上可以把Model Editing当作是Post-training，但存在的挑战是数据过少，Model Editing通常只需要一条数据.
<img src="https://i-blog.csdnimg.cn/direct/0399b05dbd3243638999d759769634da.png">

### 2. 评估Model Editing的三个维度
- `Reliability`：即达成编辑目标，所需问题有所需答案
- `Generalization`：在输入有细微的改变（语义完全不变）时，输出结果也应按照目标改变.
- `Locality`:其他无关输入不应因编辑而答案被修改.
<img src="https://i-blog.csdnimg.cn/direct/b54a0c541ce2422586343accfc7a5b47.png">

#### 2.1 Generalization的界定
Generalization由可以分为三方面：
- Paraphrase：即细微改动的同义句
- Reverse：通常是该知识的主客反问，如编辑A是B，则看能否由B再推出A.
- Portability：即模型编辑后能否把新知识迁移到其他任务中，如编辑A是B，则看能否在B相关的句子中代入A.
<img src="https://i-blog.csdnimg.cn/direct/bfff679c34ba41cd9d3e498b95d8d8ac.png">

- 用下图来总结：
<img src="https://i-blog.csdnimg.cn/direct/0937de522c2d42b09c67ffdac280e227.png">

### 3. Model Editing常见方法
常见方法分为不改参数和改变参数：
#### 3.1 不改参数（`IKE`）
直接告诉模型新知识会与模型掌握知识相悖，导致模型不相信.需要通过IKE（`In-context Knowledge Editing`）的方法，即给模型示例新知识使用方法.
<img src="https://i-blog.csdnimg.cn/direct/8f5ba4971d074be8819e0b6095a405ed.png">

#### 3.2 修改参数
##### 3.2.1 人类决定的编辑（`ROME`）
- 可以采用`Rank-One Model Editing`的方法，大致内容为找出类神经网络中与需编辑知识相关部分，再手动修改该部分的参数.
<img src="https://i-blog.csdnimg.cn/direct/38de4871db924a9e94aa3081adbd7143.png">

- 那如何找出类神经网络中与知识相关的部分呢？以图示为例：把“The Space Needle”部分的token遮住（在token embedding中加入noise或者置为zero vector）由于输入改变，每一层的embedding都发生改变，把原先的embedding置换到遮挡后相同位置的embedding，观察模型的输出，如果输出原有答案则说明该embedding与知识有关联.
<img src="https://i-blog.csdnimg.cn/direct/f7794c91c6554ba0a1639c0ac7f2cbf3.png">

- 接下来便是修改：在一个Transformer里面的FeedForward（MLP）的最后一层layer的参数进行编辑，就会改变加入residual stream的输入，从而改变该layer的输出
<img src="https://i-blog.csdnimg.cn/direct/6fb5bd18b9b2407783ece5592433bf3d.png">

- 除了给定输入$k^*$（考虑多个输入k的平均，可以增加generalization能力），把原有的输出由$v$修改为$v^*$外（实现Reliability），我们还希望增加编辑后的Locality能力.
<img src="https://i-blog.csdnimg.cn/direct/316ccb7ae7e942d19fe833a3c2989c63.png">

- 如下是写成数学形式的过程，其中矩阵K为不希望被修改的知识.
<img src="https://i-blog.csdnimg.cn/direct/ece91f8fdc1f4ea685e5c1d49d48eec5.png">

##### 3.2.2 人工智能学习如何编辑
- 利用人工智能来代替人类角色，实际上是让另一个人工智能来决定如何编辑.引入编辑模型（`Hypernetwork`）,记作$\phi$,待编辑模型记作$\theta$,编辑模型在接收到指令后，会输出与待编辑模型参数大小一致的向量$e$传递给待编辑模型.
<img src="https://i-blog.csdnimg.cn/direct/9b922c1c1f994317b315a053f70083b4.png">
- 那如何训练Hypernetwork呢？我们希望编辑模型能输出参数向量$e$,但我们往往不能知道其实际是什么样的.因而有另一种训练方式，即把两个模型合并看作一个类神经网络，此时参数向量e就是类神经网络中某一层hidden state的输出，我们只需要训练模型，让模型达到输出目标就行.
<img src="https://i-blog.csdnimg.cn/direct/d704684ccaa949f5b9495869f756a931.png">
- 该方法训练和测试流程如下：训练时提供实现Reliability和Generalization的资料（x）和Locality的资料（u）.在测试时只需关注输出，不需担心参数向量e对其他问题造成影响，因为在大量训练资料情况下，模型可能自动学会了Locality的能力.
<img src="https://i-blog.csdnimg.cn/direct/1b37a488851c4ae28175f559f19863a4.png">
- 但上述方法的训练其实是困难的，因为需要参数量巨大的模型.可以考虑利用Loss计算出Gradient Descent，再通过一个神经网络来输出参数向量e
<img src="https://i-blog.csdnimg.cn/direct/bd25aee8ec504602aebca833097c91ed.png">
- 可以利用`MEND`方法对Gradient Desent（g）的大小问题进行进一步优化，即把g分解为$u$和$v^T$,再把它们传给一个类神经网络得到$\hat{u}$和$\hat{v^T}$,把得到的二者相乘得到一个矩阵，把该矩阵当作参数向量e来更新待编辑模型.
<img src="https://i-blog.csdnimg.cn/direct/f78699ef9373436a9dd45f8e586b6ab2.png">