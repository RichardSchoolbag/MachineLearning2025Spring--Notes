### 10 语音语言模型的发展
***
### 目录
- [10 语音语言模型的发展](#10-语音语言模型的发展)
- [目录](#目录)
- [1.语音语言模型简介](#1语音语言模型简介)
- [2.语音如何转化为token sequence](#2语音如何转化为token-sequence)
  - [2.1 不切实际的做法](#21-不切实际的做法)
  - [2.2 tokenization的评估](#22-tokenization的评估)
    - [2.3 生成token的两大方式](#23-生成token的两大方式)
- [3.Detokenization相关策略](#3detokenization相关策略)
  - [3.1 token的粗细分类](#31-token的粗细分类)
  - [3.2 解决streaming问题](#32-解决streaming问题)
  - [3.3 为什么需要discrete tokens?](#33-为什么需要discrete-tokens)
- [4.以文字模型作为语音模型的Foundation Model](#4以文字模型作为语音模型的foundation-model)
  - [4.1 以语音来训练语音模型的效率](#41-以语音来训练语音模型的效率)
  - [4.2 反其道而行之——训练文字模型语音能力](#42-反其道而行之训练文字模型语音能力)
  - [4.3 考虑文字的语音tokenization](#43-考虑文字的语音tokenization)
- [5.训练和实现语音模型](#5训练和实现语音模型)

### 1.语音语言模型简介
- 语音语言模型实现的功能是：听懂语音，输出语音.语音语言模型的挑战性更大：相较于纯文字资料，语音资料所包含的信息更多，如内容、语者、情绪、环境等.
<img src="https://i-blog.csdnimg.cn/direct/1eaed31de8e0465eb6396c872b42557c.png">
如下为一些常见的语音语言模型
   - ChatGPT voice mode
   - Gemini Live
   - Moshi
   - GLM-4-Voice
   - Step-Audio
   - Qwen2.5-Omni
   - Kimi-Audio
   - SpeechGPT
   - Sesame
- 其基本实现功能与文字模型类似，额外需要的是将语音资料进行Tokenization转化为token后再进行训练，同时输出内容经过Detokenization后才能转化成语音
<img src="https://i-blog.csdnimg.cn/direct/2862a0fe890745adab4f8183c2be6d23.png">

- 训练过程同样与文字模型类似，可经历Pretrain、SFT、RLHF.
<img src="https://i-blog.csdnimg.cn/direct/6d4d6148e6784aaca858a3d6fe1b7337.png">

### 2.语音如何转化为token sequence
#### 2.1 不切实际的做法
- 一个不切实际的做法是：将语音信号通过语音辨识模型ASR后转化为文字，再传递给一个纯文字模型，文字模型的输出经过生成语音模型TTS后转化为语音.但显而易见这种做法会忽略上述提到的诸多除文字之外的信息.
<img src="https://i-blog.csdnimg.cn/direct/fcec8f84789546b795fb456264c417ce.png">

- 也有一个较为极端的做法：由于声音讯号是由一个个取样点组成的，我们通过把每个取样点当作一个token，这样就不会错过任何信息.当坏处是，如今一秒钟声音讯号的取样数是8k，意味着一分钟的声音讯号就要有近50w个token，数据量过于庞大.
#### 2.2 tokenization的评估
精确评估tokenization的方法是训练模型，但过于耗费资源，下面介绍两种benchmark来评估：  
- `Codec-SUPERB`：先把声音讯号经过tokenizer得到一串token，再经过detokenization还原回声音讯号.对比两组声音讯号的品质和在不同任务（如情绪等）上的表现.
- `DASB`：把声音讯号经过tokenization后直接检测这些token中的资讯，看能不能训练出对应任务的模型（如训练出文字辨识模型、情绪辨识模型等）.
<img src="https://i-blog.csdnimg.cn/direct/5bd3f2fc0df04cc4b84cf7503e7b749b.png">

##### 2.3 生成token的两大方式
- `SSL`：使用现成的encoder（Speech Self-Supervised Model）在收到一段声音输入后，每0.02s采样（不同模型有差别）得到一个vector sequence.而向量是连续的，我们需要的是离散的token sequence，可经历以下步骤得到：
  - `Quantization`:语音讯号平滑，相邻讯号可能很类似，可以把它们进行归类至同一个token.
  - `Deduplicate`：把重复的token给去除.
  - `BPE(Byte Pair Encoding)`：通过特定算法，找出常常相邻出现的token，用一个新的token来表示.
    
<img src="https://i-blog.csdnimg.cn/direct/59e6ab0c283a4b5483822a469205c646.png">
该方法在得到token sequence之后，便可以对detokenization model进行训练了.
- `Neural Speech Codec`：该方法中，tokenizer和detokenizer一起训练，即给定声音讯号，经过tokenizer和detokenizer所得到的新的声音讯号，我们希望越接近原信号越好.该方法与一般的autoencoder的训练方法类似.
<img src="https://i-blog.csdnimg.cn/direct/c6f30743be8a411695357e7a53d3955e.png">
- 总结该两种办法：其中SSL产生的token叫作`Sematic token`，而Neural Codec产生的叫作`Acoustic Token`.前者可能因为命名而有些歧义，其实际token类似于音标，不具有捕捉语义的相近性.
<img src="https://i-blog.csdnimg.cn/direct/2cd21f6cd7e944c595e179cc20465f96.png">
- Neural Codec往往是一段声音抽取多组token来表示不同资讯（`RVQ Residual Vector Quantization`），我们应该选取所有token.
<img src="https://i-blog.csdnimg.cn/direct/0d2a4b9829624c17b01f760d364099b2.png">

### 3.Detokenization相关策略
#### 3.1 token的粗细分类
- 可以以粗细把token分为不同层级，想要产生多组toke可以让一个LLM依次生成三组token，最先的是较粗的Coarse Token（如文字内容），最后的是较细的Finer Token（如韵律等），其中AudioLM和VALLE运用这种方法.
<img src="https://i-blog.csdnimg.cn/direct/6e5620137e154b4da5bb18fb3eec60a2.png">
- 上述方法为一个模型产生三组token，同样地，我们也可以让不同架构的LLM模型产生不同类的token，由粗的token逐步产生细的token.
<img src="https://i-blog.csdnimg.cn/direct/0c2fbbc63dfa4d2dbd301aaddb38c6d8.png">

#### 3.2 解决streaming问题
- 这种由粗token一直生成到细token的方法需等待所有token生成后才能detokenization，导致无法实现streaming的输出，即无需等待，模型一边生成一边输出. 一种解决办法是可以**依次**输出各自第一个coarse token、fine-grain token、finer token.就可以直接开始detokenization.
<img src="https://i-blog.csdnimg.cn/direct/5976932bf8ff47fcbfbe49916f7262ac.png">

- 但**依次**产生会导致token sequence变长，我们希望模型能**同步**直接产生三种token.但细token往往由粗token得来，无法实现同步输出.我们采用`Acoustic Delay`的方法，第一步先产生第一个coarse token，第二部产生第二个coarse token和第一个fine-grain token，第三步产生第三个coarse token、第二个fine-grain token和第一个finer token，按此进行类推.
<img src="https://i-blog.csdnimg.cn/direct/769d48b135d7413f9ce18ca7b0e1cca8.png">
- 同步产生token还可以使用两个transformer来解决：`Temporal Transformer`每次传一个向量给`Depth Transformer`，后者同步产生三种不同的token.
<img src="https://i-blog.csdnimg.cn/direct/66c4218837ad4fe386a9850e23f8418a.png">

#### 3.3 为什么需要discrete tokens?
- 在上述我们都采用离散的tokens，为什么不能采用连续的向量？ 对于语音语言模型来说，输入是离散token反而可能会丢失某些讯息，因而在输入方面离散的tokens相较于连续向量没有好处.但在输出方面，其具有优势.
<img src="https://i-blog.csdnimg.cn/direct/a6979f4866f5404d84f110f98284e266.png">

- 假设生成连续的向量，模型要做的任务是预测应该输出什么vector让其仍连续,如根据训练资料有两个标准答案（如图蓝色和绿色），模型在训练时会想要离蓝色和绿色都更近，输出可能变成二者的平均，这不是我们想要的，我们想要的是输出蓝色或绿色中的任意一种.（该方法的优化方法：特别设计Loss Function，便可以使用连续的向量，在图像和语音领域已证明可行）
<img src="https://i-blog.csdnimg.cn/direct/f4b000cf26e44f97a0e52a048f92e7f9.png">
- 而如果用离散的tokens，模型生成的是一个概率分布，有效避免了产生折中的结果的问题.
<img src="https://i-blog.csdnimg.cn/direct/fb738b9dfd294d8888c00272e9665229.png">

### 4.以文字模型作为语音模型的Foundation Model
#### 4.1 以语音来训练语音模型的效率
- 单纯以语音来训练模型的效率特别低. 假定有1 million的语音资料，其所具有的token也只有6B左右，而如今的文字模型LLaMA 3预训练的资料便达到了15T，而同样大小的语音资料的时长达到了28.5w年.
<img src="https://i-blog.csdnimg.cn/direct/81f0098bc7de4df89769b5c743417bf0.png">

#### 4.2 反其道而行之——训练文字模型语音能力
- 我们可以反过来，教文字模型语音相关的资讯，以文字模型来打造语音模型.
<img src="https://i-blog.csdnimg.cn/direct/73a5f04475644412874a200c19a105fd.png">
- 如今一个主流的想法是，由于文字模型本身就可以产生文字，所以我们让语音模型在产生语音时既产生语音又产生文字，文字作为语音的辅助，使生成更加稳定.称作语音和文字的Hybrid Decoding.
<img src="https://i-blog.csdnimg.cn/direct/d98545720e17461b9b3dae10592bbd74.png">

- 如何同时输出语音和文字呢？如若先输出文本，再输出语音对应的一系列token，会做不了即时的问答.而若先产生一个文字的token接下再产生对应语音的token的话，这意味着有语音和文字之间的微调，则训练时需要很多精确的资料来进行alignment才能达到很好的效果.
<img src="https://i-blog.csdnimg.cn/direct/a486b6f208b846658d459046c63dea2e.png">

- 参考之前tokenization的方法，把文字看作是一个特殊的token，能否同时在每一步产生一个文字的token和一个语音的token呢？这就需要一些生成策略.
如图为三种不同的生成策略，其中输出$\epsilon$代表没有输出token.第一种在文字token输出完后一直输出$\epsilon$，该策略使得前半段会同时产生语音和文字，文字生成结束后再把语音输出，Mini-Omni采用该策略.第二种在输出文字token后产生固定数目的$\epsilon$，而由于文字的token和语音token数量关系不是固定的，因而也需要语音token在适当时机也产生$\epsilon$，LLaMA-Omni使用该策略.第三种方法则是让模型自己决定如何让文字输出和语音输出一样长，模型通过预测来产生$\epsilon$，Moshi采用该策略.
<img src="https://i-blog.csdnimg.cn/direct/181b1c47b16f4b638b2436ccf7f63dbf.png">

#### 4.3 考虑文字的语音tokenization
介绍考虑文字的语音tokenization方法——TASTE（`Text-Aligned Speech Tokenization and Embedding`）.核心想法是：产生的语音的token只保留文字以外的资讯，降低其复杂度.
- 实现该方法的难点是如何使得文字token和语音token长度一致.我们需要特殊的tokenizer，根据输入的语音信号中有几个文字token就生成几个语音token.
<img src="https://i-blog.csdnimg.cn/direct/adb7a3fe244e4d0cae512f85a5d8e173.png">

- 具体过程：一段声音信号分别经过一个ASR和一个Pre-trained Encoder，前者能识别该声音中有多少个文字token，后者对其输出的不同的layer抽取不同的向量，会抽出两排向量.接下来有一个具有少量自注意力层的Aggregator，将ASR所得到的文字token当作query，把Encoder抽出来的一排向量当作key，一排当作value. query会对key进行attention，之后把value作weighted sum.这样就可以实现输入一个文字token，aggregator产生一个对应的语音token.最后把文字token及其对应的语音token输入detokenizer，便可以还原声音讯号.
<img src="https://i-blog.csdnimg.cn/direct/058247a60e6b4551bc112afec57868f5.png">
- 通过将快语速语音产生的token与慢语速产生的token相互置换，生成出来的语音在对应位置会发生改变，证明这些语音token中包含如何念语音的信息.
<img src="https://i-blog.csdnimg.cn/direct/2132960c0e4b48b8a7b49d01a6c69f89.png">

### 5.训练和实现语音模型
- 如何训练？ 让大量语音资料通过ASR跑出其对应的文字token，在经过TASTE这个tokenizer1就可以抽出每一个文字token对应的语音token，接下来就可以训练一个语音的language model.
<img src="https://i-blog.csdnimg.cn/direct/0a1f98f2ab7f4aab825a05013ad063e1.png">
- 有了普遍的模型后如何变成能真正互动的语音模型？可参考文字模型：在大量网络资料训练出好的语音语言模型后，拿大量不同人与人之间的对话来做Supervised Fine-Tuning.同时，为了避免forgetting原有文字模型的能力，可以让模型自行产生对话并输入.
<img src="https://i-blog.csdnimg.cn/direct/d1d094c428ad41ba930d63d6892c0479.png">