### 04 Transformer的竞争者们
***
### 目录
- [04 Transformer的竞争者们](#04-transformer的竞争者们)
- [目录](#目录)
- [1.需要解决的问题](#1需要解决的问题)
- [2.RNN与Self-Attention](#2rnn与self-attention)
  - [2.1 RNN Style](#21-rnn-style)
  - [2.2 Self-Attention Style](#22-self-attention-style)
  - [2.3 RNN训练时平行的可能性](#23-rnn训练时平行的可能性)
  - [2.4 Linear Attention的运作](#24-linear-attention的运作)
  - [2.5 Linear Attention的问题与改进](#25-linear-attention的问题与改进)
- [3.Mamba](#3mamba)


### 1.需要解决的问题
- 在语言模型中，每一个输出$y_t$都只能考虑于其左边的混合输入（$x_1-x_t$），如何实现该功能？下面介绍RNN和Self-Attention两类方法
<img src="https://i-blog.csdnimg.cn/direct/15aab5dac1ae4cc9845c614dc981310b.png">

### 2.RNN与Self-Attention
#### 2.1 RNN Style
- RNN流广义写法可以由两个函数表示：
$$
H(t)=f_{A,t}(H_{t-1})+f_{B,t}(x_t)\\
y_t=f_{C,t}(H_t)
$$
其中，H可为矩阵或者向量.$f$的下标$t$代表其会随时间变化，可令输入$x$与$f$有关来实现.该引入门控机制的模型又叫作`LSTM`
<img src="https://i-blog.csdnimg.cn/direct/baf9866b61cc4fc5839161b33bde3f7c.png">
同时，该模型可以与AI Agent的Memory机制类比，H<sub>0</sub>即为`Memory`，$f_{B,t}$即为`Write`，$f_{C,t}$即为`Read`,$f_{A,t}$即为`Reflection`  

- RNN Style具体流程图(BOS为Begin Of Sentense)
<img src="https://i-blog.csdnimg.cn/direct/5adc6366873f4c5d84829bf28eb85d4c.png">

#### 2.2 Self-Attention Style
- 基本原理：输入的每一个vector $x$都乘上三个不同的transform，制造出三个vector $v,k,q$. 假设要计算$y_t$,则把$q_t$与每一个位置的$k$算内积（`inner product`），得到$\alpha_{t,i}$(可称为attention的weight),再把$\alpha$进行softmax得到$\alpha\prime_{t,i}$,再把各个$\alpha\prime$与其对应的$v_i$做相乘，再做weighted sum就能得到$y_t$
<img src="https://i-blog.csdnimg.cn/direct/e575541cd69c4eab959a9ad5cfc4f689.png">

- Self-Attention Style具体流程图
<img src="https://i-blog.csdnimg.cn/direct/217741883e524716bc49e0c88c98d960.png">

>##### RNN和Self-Attention对比
>随着input的Sequence越长，Attention需要的运算量越大，对memory的需求也越大.而CNN每步的运算量固定，memory只需上一个H，需求量较小.
>而Attention的好处：可以对完整输入**并行计算**，**平行**产生每一个时间点语言模型的预测结果（即下一个token）.同时，attention还可以有效利用GPU的效能
#### 2.3 RNN训练时平行的可能性
- 将$H_1$->$H_t$式子列出，为了简化计算，不妨设$f_{A,1}(H_0)=0$,将式子逐次代入，可得到$H_t$是由多个$f_{A,t}$所嵌套的式子.
<img src="https://i-blog.csdnimg.cn/direct/d25bf51c62ad4069a24cec72e573a03f.png">
- 考虑继续简化式子，不妨舍弃`Reflection`部分，即把函数
$$
H(t)=f_{A,t}(H_{t-1})+f_{B,t}(x_t)\\
化为\\
H(t)=H_{t-1}+f_{B,t}(x_t)\\
$$
得到如下式子
<img src="https://i-blog.csdnimg.cn/direct/d2c3f85cf1e04065a001f8c4e0d72be2.png">
- 进一步化简，设$H_t$为$d\times d$的矩阵，则$f_{B,t}(x_t)=D_t$,再把$f_{C,t}(H_t)$看作是$H_t$与向量$q_t$相乘，其中$q_t=W_Q\times x_t$
可得：
<img src="https://i-blog.csdnimg.cn/direct/23ca6cbad8104d0c9aa95029e1e830db.png">
- 再视$D_t=v_t \times k_t^T$,其中$v_t=W_v \times x_t$,$k_t= W_t \times x_t$ 可再次化简得：
<img src="https://i-blog.csdnimg.cn/direct/7a9c0965f6b1410d92265a07a8a46b4c.png">
- 由于$k_t^T \times q_t $ 为常数，记作$\alpha_{t,t}$,可最终化简得如下式子.这其实就是缺少了softmax的Self-Attention. 称作`Linear Attention`.即RNN移除Reflection后就变为Linear Attention
<img src="https://i-blog.csdnimg.cn/direct/56b7f1823fb6467f9f762247517992f3.png">

#### 2.4 Linear Attention的运作
- 如图，$H_{t-1}$所加的$f_{B,t}(x_t)$为一个矩阵，这个矩阵的每一个column都是$v_t$前面乘上一个scalar $k_{t,d}$. 其中$v_t$为要写入的信息，而通过设定$k_{t,d}$的值可以决定要把信息写入哪个dimension.
<img src="https://i-blog.csdnimg.cn/direct/21be35aa26564a35b272357167d38ab7.png">

- 同理，输出$y_t=H_t \times q_t$.由于$H_t$中不同信息存放在不同列中，因而可设定$q_t$来决定读取哪些信息
<img src="https://i-blog.csdnimg.cn/direct/fca60b37059e4b97b240911b87a56cf9.png">

>Linear Attention和Self-Attention都会存在**容量有限**的问题

#### 2.5 Linear Attention的问题与改进
- 由于Linear Attention缺少softmax，其记忆永远不会改变（$H_{t-1}$不做任何处理直接加到$H_t$上）
- 解决方法：引入Reflection，在$H_t$前面乘上一个$\gamma_t$,根据情况来遗忘部分信息，其中$\gamma_t由sigmoid（W_\gamma \times x_t）$得到. 称作（`Gated Retention`）
<img src="https://i-blog.csdnimg.cn/direct/02326121660547139b02f42e0b648fa5.png">
- 训练与推理流程如图（在训练中需额外引入$\gamma_t$的计算）
<img src="https://i-blog.csdnimg.cn/direct/c02b930394b74e1a908e3643957c9526.png">
- 更复杂的Reflection方式：矩阵$G_t$与$H_t$进行element-wise运算来控制哪些column 抹去/保留/减弱
<img src="https://i-blog.csdnimg.cn/direct/afd068064a3f4c9c8092b51d45927cb1.png">
### 3.Mamba
- 如图为各种Linear Attention架构
<img src="https://i-blog.csdnimg.cn/direct/602cd133e2fd4a03bdf7f65a425cb5c5.png">
- 如下两图所示，在Linear Attention架构中，Mamba表现最为优秀.Mamba是第一个在效果和推理速度上都超过Transformer的Linear Attention架构
<img src="https://i-blog.csdnimg.cn/direct/96a3e8bf7406416eb5b7511965a6d77f.png">
<img src="https://i-blog.csdnimg.cn/direct/a307184bd3824e308e6eda5739927e14.png">