### 03 大型语言模型内部运作机制
***
### 目录
- [1.一个神经元在做什么](#1一个神经元在做什么)
   - [1.1 什么是神经元](#11什么是神经元)
   - [1.2 基本过程](#12-基本过程)
   - [1.3 如何确定一个神经元在做什么](#13-如何确定一个神经元在做什么)
   - [1.4 单一神经元一般不负责具体任务](#14-单一神经元一般不负责具体任务)
- [2.一层神经元在做什么](#2一层神经元在做什么)
   - [2.1 功能向量](#21-功能向量)
   - [2.2 如何找出功能向量](#22-如何找出功能向量)
   - [2.3 如何验证找出的向量](#23-如何验证找出的向量)
   - [2.4 如何一次性找出大量功能向量](#24-如何一次性找出大量功能向量)
- [3.一群神经元在做什么](#3一群神经元在做什么)
   - [3.1 语言模型的模型](#31-语言模型的模型)
   - [3.2 如何系统化建立语言模型的模型](#32-如何系统化建立语言模型的模型)
- [4.让语言模型直接说出想法](#4让语言模型直接说出想法)
   - [4.1 语言模型的思维是透明的](#41语言模型的思维是透明的)
   - [4.2 Residual Stream](#42-residual-stream)

### 1.一个神经元在做什么
#### 1.1什么是神经元
- 在Transformer中，对于输入的token sequence（z<sub>1</sub>到z<sub>t-1</sub>），ai会通过特定的function来预测z<sub>t</sub>，输出概率分布.
<img src="https://i-blog.csdnimg.cn/direct/d98038ddcf1d4830b4f76c40b0bdc91b.png">

#### 1.2 基本过程
- Transformer内部基本构造如下，首先会把token sequence转化为vector sequence，每个token对应一个向量，这个转化过程称为`Embedding`
最后一层的最后一个向量会被用于产生概率分布，这个过程称为`Unembedding`
<img src="https://i-blog.csdnimg.cn/direct/1e88077412984a88926ee7e1cd1ba828.png">

- Transformer中只处理单一token的layer原理如下，输出向量的每一个dimension都由输入向量所有数值进行weighted sum，再经过一个activation fuction（一般用ReLU）得到输出.输出向量的每一个数值就叫一个神经元（`neuron`）的输出，其中间的转换就叫作一个神经元neuron
<img src="https://i-blog.csdnimg.cn/direct/39609f742c554f8f84dbb0f5f4a57fcd.png">

#### 1.3 如何确定一个神经元在做什么
一般有三个步骤：
- 若观察到某个行为发生时，某个神经元都会启动（经过ReLU后输出大于0），则能说明该行为可能与这个神经元**有关**. （区别因果关系和相关关系）  
- 若移除该神经元（输出值设为其所有情况输出的平均值），该模型就不产生某种行为，也可以说该行为与这个神经元**有关**（输出不能设为0，设为0可能导致其他神经元启动）
- 根据不同启动程度来观察某种行为的执行程度
<img src="https://i-blog.csdnimg.cn/direct/9a888ff6ee984d9fa16840cb07bb03fc.png">

#### 1.4 单一神经元一般不负责具体任务
- 若一个神经元对应一个任务，所能实现的功能十分有限，一般是多个神经元共同负责一个任务. 假设有4096个神经元，即使神经元只有开关特性，也仍然有2<sup>4096</sup>种可能性，十分可观
<img src="https://i-blog.csdnimg.cn/direct/f1760fd2e4b9420fae27bb45835bfa96.png">

### 2.一层神经元在做什么
#### 2.1 功能向量
- 当模型执行某个功能时，启动的神经元的数值排列起来可以视为一个向量，称为功能向量. 设第n层layer的输出（`Representation`）负责该功能，当representation与功能向量越接近，越可能执行该功能
<img src="https://i-blog.csdnimg.cn/direct/f1bb0bc4cd9c4434afa4d24ae423ee05.png">

#### 2.2 如何找出功能向量
- 以找出拒绝向量为例.在拒绝情况下，输出为拒绝向量+其他向量（不做xx），而在同意情况下，输出为同意向量+其他向量.将二者平均值的一半相减，即可抵消其他向量，并可能得到拒绝向量
<img src="https://i-blog.csdnimg.cn/direct/b2d2cd29fe9d468dbbddaafa00cca305.png">

#### 2.3 如何验证找出的向量
- 仍以拒绝向量为例.把找出的向量加到对应的representation上，若正常的问题模型反而输出为拒绝，则该向量很可能带有拒绝功能
<img src="https://i-blog.csdnimg.cn/direct/80ff77ea833247eaaa3eb1dffc9441a1.png">

#### 2.4 如何一次找出大量的功能向量
- 一个representation（图中h）可以看作是功能向量（图中v）的线性组合（`Linear Combination`）再加非功能向量的部分（图中e）
<img src="https://i-blog.csdnimg.cn/direct/fadc4c135bdd49bfa62afb6978d78662.png">
- 我们希望h向量尽量只与功能向量v有关，所有非功能向量e越小越好，给定Loss Function（如图第一项）. 可以通过假定每个向量V<sub>k</sub>在第k维是1，其他均为0，然后通过线性组合使得每一个e都是零向量，使得Loss最小. 但这种方法使得单个功能向量负责某件特定事项，问题变为复杂（与单个神经元负责单项任务类似）.因而我们优化Loss Function，我们希望每次选择的功能向量尽量少，即$\alpha$尽量为0，如图第二项. 可以用SAE（`Sparse Auto-Encoder`）的方法求解.
<img src="https://i-blog.csdnimg.cn/direct/5863ca664e8e4a558dde287a82fee52a.png">

### 3. 一群神经元在做什么
#### 3.1 语言模型的模型
- 虽然语言模型本身即为模型，但其本身还是十分复杂，需引入模型的模型来方便解析，其保有原事物特征的衡量又叫作`Faithfulness`
<img src="https://i-blog.csdnimg.cn/direct/8c38c1dde62542c0a4b5ff204dbfee71.png">

- 以下图的prompt为例，主语得到一个representation（x），再从is located in这个关联性词汇会产生一个Linear Function（即矩阵W和向量b），再与x进行组合得到y，再unembedding就能得到输出
<img src="https://i-blog.csdnimg.cn/direct/2c80f7d8ebb54b68a1e6d2230c99e18a.png">

#### 3.2 如何系统化建立语言模型的模型
- 把语言模型不断做`Purning`来简化模型（如移除几个自注意力层、移除某几个神经元等），简化到一目了然且目标任务不变，即给定输入能给出需要的输出，这样就建立了"模型的模型"(`Circuit`)
<img src="https://i-blog.csdnimg.cn/direct/b2bf28c3441d457d9d6f08f529cebe9e.png">

### 4.让语言模型直接说出想法
#### 4.1语言模型的思维是透明的
- 在每一层layer中得到一排输出时，还会与原本的输出再加起来才得到最终的输出，称作`Residual Connection`
<img src="https://i-blog.csdnimg.cn/direct/c66c6b6f2923409f91c9156e9f30d663.png">

#### 4.2 Residual Stream
- 在如图右侧的流程图中，可以直观地感受到输入一路传递到输出，每层layer只是给输入加一部分内容，这个过程叫作`Residual Stream`.未经过最后一步softmax得出来的几率分布图叫作`Logit Lens`
<img src="https://i-blog.csdnimg.cn/direct/f4f5f42a6f6c456e8392fd65bebbb5f3.png">
- 如图，每一个加入residual stream的这些向量（V<sub>2</sub>-V<sub>D</sub>）都可以通过unembedding转化成一个token的distribution. 这就使得类神经网络编辑称为可能，对于某个问题的答案，可以减去其对应的token embedding，再加入所需的token embedding
<img src="https://i-blog.csdnimg.cn/direct/d8ce4491f0ff4044891f8f8a5f1caca7.png">
- 将每层最后位置的representation进行解析，这种方法称为`Patchscope`
<img src="https://i-blog.csdnimg.cn/direct/931d946ac7e745fda2d5a5d34bf2d124.png">