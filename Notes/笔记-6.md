### 06 生成式人工智慧的后训练与遗忘问题
***
### 目录
- [06 生成式人工智慧的后训练与遗忘问题](#06-生成式人工智慧的后训练与遗忘问题)
- [目录](#目录)
- [1.通用模型时代常见情境](#1通用模型时代常见情境)
- [2.Post Training与Forgetting](#2post-training与forgetting)
  - [2.1 Post Training方法](#21-post-training方法)
  - [2.2 所带来的问题](#22-所带来的问题)
  - [2.3 Post-Training的挑战——`Catastrophic Forgetting`](#23-post-training的挑战catastrophic-forgetting)
    - [2.3.1 影响因素的探讨](#231-影响因素的探讨)
    - [2.3.2 有成本的方法——`LoRA`](#232-有成本的方法lora)
    - [2.3.3 有效解法——`Experience Replay`（2019）](#233-有效解法experience-replay2019)
  - [2.3.4 解法类似变形](#234-解法类似变形)
  - [2.4 训练文字模型语音能力](#24-训练文字模型语音能力)
### 1.通用模型时代常见情境
- 通用模型（如已开源的LLaMa、Gemma、DeepSeek、ChatGPT）当前十分流行，它们各种基础能力都达到一定量级. 但若我们想打造具有专长的模型，则需要对通用模型做进一步的学习，这个过程称为（`Post-Training/Continual Learning`）.把Post-Training前后的模型分别称作`Foundation Model`和`Fine-tuned Model`.
<img src="https://i-blog.csdnimg.cn/direct/a48c08a849814b75b5ac833a807b8895.png">

### 2.Post Training与Forgetting
#### 2.1 Post Training方法
主要为以下三种方法：
  - Pre-train Style
  - SFT Style
  - RL Style
#### 2.2 所带来的问题
- 多个实验结果证明：Post-training后，模型的Safety Alignment会变差，可能输出有害内容，这不是我们所希望的.
<img src="https://i-blog.csdnimg.cn/direct/bcf91aa27a0c4bb5b67920181b366fab.png">
<img src="https://i-blog.csdnimg.cn/direct/29cade87fa7c4dc29a1fdb3f0cfeac62.png">

- 同时，模型在专注于特定目标进行Post-Training后，在目标任务的能力会上升，但原有的能力会下降，这同样不被希望.
<img src="https://i-blog.csdnimg.cn/direct/7093a1e06e4341fe9ff930cf1fee30f7.png">

- 以教LLaMA模型进行语音情绪辨识实验为例：如图，在一个Epoch训练后，模型能够按照要求输出JSON的格式（该能力模型本身就具有），但答案并不准确. 但当把Epoch数量增加到3，对于同一个问题，其答案是准确的，但并不按照JSON格式输出
<img src="https://i-blog.csdnimg.cn/direct/5b2e4605b35e4294a833fbed644bf01a.png">

#### 2.3 Post-Training的挑战——`Catastrophic Forgetting`
##### 2.3.1 影响因素的探讨
- 实验发现：在1B-7B的模型上，较大模型forgetting的状况并没有更轻微.(更大模型仍需探讨)  且如图所示，点集拟合近似一条斜直线，证明模型在目标任务上学习得越好，forgetting的现象就更严重.
<img src="https://i-blog.csdnimg.cn/direct/adeb5f99889c4450be14c61f17f31a74.png">

##### 2.3.2 有成本的方法——`LoRA`
- 若尝试引入LoRA来解决Forgetting问题，其代价是学习量的下降.即学习的越少，遗忘越少.该方法在较大规模的训练情况下显然不适用.
<img src="https://i-blog.csdnimg.cn/direct/3aa9418b0f4f4032864a3f35a21c1923.png">

##### 2.3.3 有效解法——`Experience Replay`（2019）
- 即在训练第二个任务时，混杂一点任务一的训练资料（不需多，5%便足够）.实验证明，这是一个可以有效防止遗忘的方法. 
- 如若只拥有训练后的模型，而没有训练资料，则可以通过输入BOS，让模型自己产生一些句子，再把这些句子当作任务一的训练资料，加到任务二中.
<img src="https://i-blog.csdnimg.cn/direct/9e037cf38ba647f18f4d1aa0950809f0.png">

- 上述解法在2019年提出.时至今日，该方法仍在沿用.在2023年的论文中，发现加入3%的Safety Alignment资料，便可以保留模型原有的能力
<img src="https://i-blog.csdnimg.cn/direct/b3f84ca5a3ed48e4b423c1e606d3f386.png">

#### 2.3.4 解法类似变形
- `Paraphrase`:即在训练时，不直接拿正确的答案进行训练，而是把答案交给模型让其换个说法后再投入训练. (可以理解为以模型视角复述出的话更接近其之前的训练资料)
- `Self-Output`：让模型产生多个答案，只挑选正确的答案进行训练（与RL极其类似，因而有还需进一步验证的猜想：RL base的模型发生forgetting的概率低）
<img src="https://i-blog.csdnimg.cn/direct/5cad05a0b4dc42d3bb29bf8efbd26f5b.png">

#### 2.4 训练文字模型语音能力
- 由Self-Output方法，可以用于训练文字模型语音能力. 语言模型本身听不懂语音，但可以人为地给声音讯号进行标注，将各种声音特征用文字描述出来输入给语言模型，再输入指令（如What can you hear？），把文字模型的输出当作语音模型的目标来训练，可以有效避免语音模型遗忘语言模型的能力.
<img src="https://i-blog.csdnimg.cn/direct/413d6ccd17d7453b936dd7201df07ea7.png">
